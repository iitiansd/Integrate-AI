def get_qa_reflection_stage1_prompt():
    prompt = """
    "You are a quality assurance engineer. Trying to help QE generate test cases using LLMs."
    "User has uploaded tech design document in PDF format and then is trying to get full list of test cases in predefined JSON format."
    "Quality check LLM answer and generate critic/feedback for the LLM. If you think LLM answer is complete say 'Finished', else generate followup question for the LLM chat session using feedback you have generated. Ensure you only get the genuine information. Do not work with hypothetical, speculative or fake information. You can ask for reattempting the task eventhough it failed as multiple attmps are sometimes needed."
    "JSON output should be legitimate and must not contain any non parsable elements like code comments, etc."
    """
    return prompt

def get_qa_sub_reflection_stage1_prompt():
    prompt = """
    "You are a quality assurance engineer. Trying to help QE generate test cases using LLMs."
    "User has uploaded tech design document in PDF format and then is trying to get full list of test cases in predefined JSON format."
    "Quality check LLM answer and generate critic/feedback for the LLM. If you think LLM answer is complete say 'Finished', else generate followup question for the LLM chat session using feedback you have generated. Ensure you only get the genuine information. Do not work with hypothetical, speculative or fake information. You can ask for reattempting the task eventhough it failed as multiple attmps are sometimes needed."
    "JSON output should be legitimate and must not contain any non parsable elements like code comments, etc."
    "In case of any server error, prompt AI to retry as a follow-up question"
    """
    return prompt


def get_qa_reflection_stage2_prompt():
    prompt = """
    "You are a quality assurance engineer. Trying to help QE generate test cases using LLMs."
    "User has uploaded tech design document in PDF format and then is trying to get full list of test cases in predefined JSON format."
    "Please ensure that complete JSON list for test case details is generated and its not left incomplete."
    "Quality check LLM answer and generate critic/feedback for the LLM. If you think LLM answer is complete say 'Finished', else generate followup question for the LLM chat session using feedback you have generated. Ensure you only get the genuine information. Do not work with hypothetical, speculative or fake information. You can ask for reattempting the task eventhough it failed as multiple attmps are sometimes needed."
    """
    return prompt


def get_qa_reflection_stage3_prompt():
    prompt = """
    "You are a quality assurance engineer. Checking quality of automated extraction generated by LLMs."
    "User is trying to get full information for some web API endpoint from the relevant documents."
    "Quality check LLM answer and generate critic/feedback for the LLM. If you think LLM answer is complete say 'Finished', else generate followup question for the LLM chat session using feedback you have generated. Ensure you only get the genuine information. Do not work with hypothetical, speculative or fake information. You can ask for reattempting the task eventhough it failed as multiple attmps are sometimes needed."
    "In case of failure or error, you can ask for reattempting the task eventhough it failed as multiple attmps are sometimes needed."
    """
    return prompt

def get_test_case_type_prompt_soft(test_case:dict):
    prompt = f"""
    Can you look at following test case and let me know whether it should be tested at end-toend level or componant level based on following guide line:

    # Instructions:
    - Used TD and FRD documents attached to understand the test case
    - You also have expert knowledge of Celigo Product KB and all the back end microservices, use it where ever needed
    - Plan and execute your actions

    # Test Case:
    {test_case.get('Title')}

    # Pre Conditions:
    {test_case.get('Pre_Conditions')}

    # Guideline:
    Test cases are segregated into component tests or end-end tests based on how they can be automated
    For a test case to be a component test -
    i) The functionality that we are trying to test must be handled within the service. It shouldn’t need any upstream or downstream services.
    ii) We would then see if there is any api available, either the rest api or websocket to test it within the service.
    For a test case to be a functional test -
    i) Anything that needs to be validated with flow runs
    ii) Functionality related to mapping, transformations, user level settings, integration of multiple services

    # Expected Output:
    Provide the output in the following strict JSON format:
    ```json
    {{
        "Test_Type": "End-to-end or Component",
        "Reason": "A clear and concise explanation of why the test case falls into the chosen category."
    }}
    ```

    # Important Notes:
    - Ensure the JSON output is valid and strictly adheres to JSON standards.
    - Do NOT include any code comments (`//` or `/* */`) inside the JSON output.
    - Every line in the output must be valid JSON—no inline comments, explanations, or additional text outside of the JSON structure.
    - Avoid placeholders requiring runtime evaluation, such as string concatenation (`"some" + "string"`) or function calls.
    - Do NOT use ellipsis (`...`) as a placeholder for fields, arrays, or objects. JSON must contain explicit values in all fields.
    - The output must be a clean, directly parsable JSON object.
    """
    return prompt

def get_test_case_type_prompt_hard(test_case:dict):
    prompt = f"""
    You are tasked with deciding whether a backend test case should be tested at Component level or End-to-End level for Celigo's Integrator.IO.

    # Guideline to classify a testcase:
    Test cases are segregated into component tests or end-end tests based on how they can be automated
    For a test case to be a component test -
    i) The functionality that we are trying to test must be handled within the service. It shouldn’t need any upstream or downstream services.
    ii) We would then see if there is any API available, either the REST API or WebSocket to test it within the service.
    For a test case to be a functional test -
    i) Anything that needs to be validated with flow runs
    ii) Functionality related to mapping, transformations, user level settings, integration of multiple services


    You must strictly follow this sequence:

    # 1. Step: Plan
    - Based on the Test Case and Instructions, generate a Plan.
    - The Plan must describe:
    - What tools you will use (FRD, TDD, Celigo Product KB, Microservices Docs, GitHub).
    - What information you intend to retrieve from each tool.
    - IMPORTANT: You must plan specific queries you intend to run.

    # 2. Step: Execute
    - Based on your Plan, actually make tool queries.
    - After each tool query, log your findings into your memory (scratchpad).
    - You MUST call at least:
    - Celigo Product KB once
    - Microservices Documentation once
    - GitHub Repository **(please find impacted Git Repo from technical design document and use them for searching)**

    # 3. Step: Analyze
    - Summarize what you found from each tool.
    - Answer the Critical Thinking Questions:
    - Is the functionality contained within a single microservice?
    - Does it require orchestration across services (e.g., flow runs, mappings)?
    - Are direct APIs available?

    # 4. Step: Decision
    - Based on findings and self-analysis, conclude:
    - "Component" or "End-to-End"
    - Provide detailed Reasoning.
    - Explicitly cite what information led you to your conclusion.

    # Test Case:
    {test_case.get('Title')}

    # Pre Conditions:
    {test_case.get('Pre_Conditions')}   
    
    # Output Format:
    Return your answer as JSON:

    ```json
    {{
    "Plan": "What steps you planned and tools you selected.",
    "Tools_Used": ["Tool1", "Tool2", ...],
    "Findings": "Summary of information gathered from tools.",
    "Self_Analysis": "Answers to critical thinking questions.",
    "Test_Type": "Component" | "End-to-End",
    "Reason": "Detailed reasoning citing tool findings."
    }}
    ```
    # Important Notes:
    - Ensure the JSON output is valid and strictly adheres to JSON standards.
    - Do NOT include any code comments (`//` or `/* */`) inside the JSON output.
    - Every line in the output must be valid JSON—no inline comments, explanations, or additional text outside of the JSON structure.

    """
    return prompt


def get_general_test_case_generation_prompt(scenario):
    prompt = f"""Generate a comprehensive list of test cases based on the test scenario {scenario} by utilizing detailed information from technical design documents and the Feature requirements document. 
                Refer to the product Knowledge base when additional clarification is needed.

                # Note:                       
                - Just generate requested JSON without any other details, observations or instructions.Do not include any additional commentary.

                # Steps

                1. **Analyze Test Scenario**: Thoroughly understand the test scenario to identify the core functionalities and potential edge cases.
                2. **Review Documentation**: Examine the technical design document, Feature requirements document and shema docs for routes, payloads, response, error conditions and additional details that inform the test cases.
                3. **Consult Knowledge Base**: If unclear aspects arise, access the product Knowledge base to resolve any uncertainties.
                4. **Develop Test Cases**: Create detailed test cases based on gathered information. Each test case should include:
                - **Title**: A brief, descriptive name for the test case.
                - **Type**: The nature of the test (e.g., Functional, Integration, Regression).
                - **Pre_Conditions**: Any preconditions or setup required before the test can be executed. **Produce detailed steps**.

                ### **Important JSON Output Requirements**
                - **Ensure the JSON output is valid and strictly adheres to JSON standards.**  
                - **Do NOT include any code comments** (`//` or `/* */`) inside the JSON output.  
                - **Every line in the output must be valid JSON**—no inline comments, explanations, or additional text outside of the JSON structure.  
                - **Avoid placeholders requiring runtime evaluation**, such as string concatenation (`"some" + "string"`) or function calls.  
                - **Do NOT use ellipsis (`...`) as a placeholder for fields, arrays, or objects.** JSON **must contain explicit values** in all fields.                 
                - **If any expressions or other constructs are required, they must be enclosed as strings** to prevent JSON parsing issues.     
                - **The output must be a clean, directly parsable JSON object.**  


                # Output Format

                Format the output as a JSON object with a key *strictly* named "test_list" containing a JSON list, with each test case containing the fields "Title," "Type," and "Pre_Condition."


                - **Example Test Case in JSON**:
                ```json
                {{
                    'test_list':
                        [
                            {{
                                "Title": "Successful User Login with Valid Credentials",
                                "Type": "Functional",
                                "Pre_Conditions": "User has a valid account with active internet connection."
                            }},
                            {{
                                "Title": "Login Attempt with Invalid Password",
                                "Type": "Negative",
                                "Pre_Conditions": "User has a valid account but enters an incorrect password."
                            }}
                        ]
                }}
                ```      
                ## Important Notes
                - Focus only on given test scenario **{scenario}**.

                # Notes

                - Ensure test cases cover a wide range of scenarios including edge cases and negative testing.
                - All test cases should be easily understandable and executable by testers without additional context.
                - Consider varying types of testing to encompass functional, negative, boundary, and regression tests.
                - Maintain consistency in the format and level of detail across all test cases.
                """
    return prompt


def get_backend_test_case_generation_prompt(test_case):
    prompt = f"""Generate comprehensive test case details for each given test case {test_case} using additional information from various sources. Refer to the technical design documentation and feature requirements document for further details. 
                Access the product knowledge base if required. Look for sample request and response information from the attached documents.

                # Note:                       
                    - Just generate requested JSON without any other details, observations or instructions.Do not include any additional commentary.

                # Steps

                1. **Review Test Case**: Identify the test case details from the documents provided.
                - Identify the route first
                - Extract the request and response details for the identified route from the documents.
                2. **Consult Documentation**:
                - Use the technical design document to gather implementation specifics.
                - Refer to the feature requirements document to understand expected behaviors and outcomes.
                - Examine attached documents for sample requests and responses schema.
                - If attached, refer to the OpenAPI specification document. All API requests and responses must strictly adhere to the definitions, parameters, and schema outlined in the OpenAPI spec.
                - Look for error conditions and edge cases to ensure comprehensive test coverage, consult attached documents for actual error codes.
                - **Review any other attached documents** (e.g., architecture diagrams, integration notes, test reports) that may provide relevant context, validations, or usage flows related to the feature or endpoint.   
                3. **Access Knowledge Base**: If further clarification is needed, consult the product knowledge base for additional insights.
                4. **Construct JSON**:
                - Extract details to form a complete test case with specified fields.

                ### **Important JSON Output Requirements**
                - **Ensure the JSON output is valid and strictly adheres to JSON standards.**  
                - **Do NOT include any code comments** (`//` or `/* */`) inside the JSON output.  
                - **Every line in the output must be valid JSON**—no inline comments, explanations, or additional text outside of the JSON structure.  
                - **Avoid placeholders requiring runtime evaluation**, such as string concatenation (`"some" + "string"`) or function calls.  
                - **Do NOT use ellipsis (`...`) as a placeholder for fields, arrays, or objects.** JSON **must contain explicit values** in all fields.                 
                - **If any expressions or other constructs are required, they must be enclosed as strings** to prevent JSON parsing issues.     
                - **The output must be a clean, directly parsable JSON object.**  

                # Output Format

                Format the output as a JSON object with a key *strictly* named "test_list" containing a JSON list, with each test case containing the following fields:
                - `Expected_Result`: [Clearly define the expected outcome based on the feature requirements and technical design.]
                - `Request_Body`: [Detail the request body structure as outlined in the technical design or sample documents.]
                - `Response`: [Provide the anticipated response structure as per sample or expected outcome.]           

                # Examples

                **Example 1:**

                Input: Test Case detail like Title, Type and Pre_conditions

                Output:
                ```json
                {{
                    "test_list": [
                        {{
                        "Expected_Result": "[Expected outcome based on source documents]",
                        "Request_Body": {{
                            "Request payload as described in the documents."
                        }},
                        "Response": {{
                            "Response structure as defined in the documents."
                        }}
                        }}
                    ]
                }}
                ```

                # Notes

                - When crafting the request body and response, ensure that the structures adhere strictly to the examples found in the respective documents.
                - Align test case outcomes with the latest feature requirements to ensure accuracy.
                - Consider edge cases where original documentation may be vague or incomplete.
                ## **Important Note:**
                - Ensure that the request body and response structures are accurately represented based on the provided documents.
                - JSON output should be legitimate and must not contain any non parsable elements like code comments, etc.
                """
    return prompt


def get_front_end_test_case_generation_prompt(test_case):
    prompt = f"""Generate UI test case details for a given Test Case **{test_case}** using the provided Feature Requirement Document (FRD), Technical Design, and Figma diagrams. 
                Ensure to identify and validate the specific workflows and form relationships accurately before generating the test steps. Include specific details about new UI fields by generating custom data-* attributes for them along with the test steps. Return the completed test case details in JSON format.

                Utilize the provided documents and diagrams to extract relevant information and construct the test case with comprehensive details.

                 # Note:                       
                - Just generate requested JSON without any other details, observations or instructions.Do not include any additional commentary.

                # Steps

                1. Analyze the Workflow Context:
                    - Cross-reference all provided documents to validate the specific form or page where the interaction occurs. 
                    - Ensure that any UI elements mentioned are correctly attributed to their respective forms.
                    - Test steps remain contextually accurate.
                    - Transitions or steps from previous pages are avoided unless specified.
                    - Validating every UI element's exact location and context
                    - Cross-check transitions between pages to confirm that test steps reflect only the intended page and user actions.
                    - Ensure clarity on transitions between forms/pages to avoid mixing workflows.


                2. Extract Features and Requirements:
                    - From the FRD, identify the fields and interactions for the specific workflow being tested.
                    - Validate field locations and their dependencies between different forms.
                3. Incorporate UI Relationships:
                    - Explicitly mention form transitions and ensure the test steps align with the exact flow of user actions.
                4. Generate Custom data-* Attributes:
                    - For new UI fields (e.g., buttons, dropdowns, or form fields), create necessary data-* attributes, such as data-test='newFieldName'.
                5. Detail Test Steps:
                    - List step-by-step UI interactions and explicitly specify data-* attributes only for new fields.
                    - Ensure each step reflects the validated user flow across forms.
                6. Determine Expected Results:
                    - Specify outcomes expected after performing the test steps, ensuring they align with the workflow and validated relationships between forms.
                7. **Construct JSON**:
                    - Extract details to form a complete test case with specified fields.

                ### **Important JSON Output Requirements**
                - **Ensure the JSON output is valid and strictly adheres to JSON standards.**  
                - **Do NOT include any code comments** (`//` or `/* */`) inside the JSON output.  
                - **Every line in the output must be valid JSON**—no inline comments, explanations, or additional text outside of the JSON structure.  
                - **Avoid placeholders requiring runtime evaluation**, such as string concatenation (`"some" + "string"`) or function calls.  
                - **Do NOT use ellipsis (`...`) as a placeholder for fields, arrays, or objects.** JSON **must contain explicit values** in all fields.                 
                - **If any expressions or other constructs are required, they must be enclosed as strings** to prevent JSON parsing issues.     
                - **The output must be a clean, directly parsable JSON object.**  

                # Output Format

                Format the output as a JSON object with a key *strictly* named "test_list" containing a JSON list, with each test case containing the following fields:
                ```json
                {{
                "test_list": [
                    {{
                "Title": "[Title of the Test Case]",
                "Type": "[Type, e.g., Functional, Regression]",
                "Pre_Conditions": "[Any setup conditions]",
                "Test_Steps": [
                    "[Step 1: Description of user interaction and generate Custom data-* attributes]",
                    "[Step 2: Continue detailed steps...]"
                ],
                "Expected_Result": "[Expected outcomes of the test]"
                }}
                ]
            }}

                ```

                 ## **Important Note:**
                 - Ensure that the request body and response structures are accurately represented based on the provided documents.
                 - JSON output should be legitimate and must not contain any non parsable elements like code comments, etc.

                # Examples

                **Example 1:**
                ### Input:
                - FRD details on feature X
                - Tech Design highlighting feature X implementation
                - Figma diagram showing new fields

                ### Output:
                ```json
                {{
                    "test_list": [
                    {{
                    "Title": "Test Case for Feature X",
                    "Type": "Functional",
                    "Pre_Conditions": "User is logged in and on the feature X page.",
                    "Test_Steps": [
                        "Step 1: Click the 'Submit' button  [data-test='submitButton'].",
                        "Step 2: Verify the 'Success' message appears."
                    ],
                    "Expected_Result": "Upon clicking the 'Submit' button, a 'Success' message is displayed."
                    }}
                    ]
                }}
                ```
                # Notes

                - Ensure all steps are detailed clearly with specific interactions.
                - data-* attributes must only be created for **new UI fields** being added.
                - Review all provided documents thoroughly to ensure accuracy and completeness of the test case.
                - Adjust the complexity of steps depending on the feature being tested.
                - Consider edge cases and handle unexpected behavior in the expected results.

            """
    return prompt