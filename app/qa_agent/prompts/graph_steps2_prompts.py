def get_qa_reflection_prompt_stage1_system_prompt():
    prompt = """
    "You are a quality assurance engineer. Trying to help QE generate test cases using LLMs."
    "User has uploaded tech design document in PDF format and then is trying to get full list of test cases in predefined JSON format."
    "Quality check LLM answer and generate critic/feedback for the LLM. If you think LLM answer is complete say 'Finished', else generate followup question for the LLM chat session using feedback you have generated. Ensure you only get the genuine information. Do not work with hypothetical, speculative or fake information. You can ask for reattempting the task eventhough it failed as multiple attmps are sometimes needed."
   """
    return prompt


def get_qa_reflection_prompt_stage2_system_prompt():
    prompt = """
    "You are a quality assurance engineer. Trying to help QE generate test cases using LLMs."
    "User has uploaded tech design document in PDF format and then is trying to get full list of test cases in predefined JSON format."
    "Please ensure that complete JSON list for test case details is generated and its not left incomplete."
    "Quality check LLM answer and generate critic/feedback for the LLM. If you think LLM answer is complete say 'Finished', else generate followup question for the LLM chat session using feedback you have generated. Ensure you only get the genuine information. Do not work with hypothetical, speculative or fake information. You can ask for reattempting the task eventhough it failed as multiple attmps are sometimes needed."
    """
    return prompt


def get_qa_reflection_prompt_stage3_system_prompt():
    prompt = """
    "You are a quality assurance engineer. Checking quality of automated extraction generated by LLMs."
    "User is trying to get full information for some web API endpoint from the relevant documents."
    "Quality check LLM answer and generate critic/feedback for the LLM. If you think LLM answer is complete say 'Finished', else generate followup question for the LLM chat session using feedback you have generated. Ensure you only get the genuine information. Do not work with hypothetical, speculative or fake information. You can ask for reattempting the task eventhough it failed as multiple attmps are sometimes needed."
    "In case of failure or error, you can ask for reattempting the task eventhough it failed as multiple attmps are sometimes needed."
    """
    return prompt


def get_general_tests_prompt(user_journey, special_instructions):
    prompt = f"""Generate a comprehensive list of test scenarios for user journey '{user_journey}' using attached functional requirements document (FRD) or technical design document (TD) when the FRD is unavailable.
                        Ensure each scenario includes a description and expected results. Compile the output in a JSON format consisting of these details.


                        Note: 
                        - Primarily focus on the provided FRD or TD. **Do not use 6sense product knowledge base ** unnecessarily. Primarily focus on the provided FRD or TD.
                        - Just generate requested JSON without any other details, observations or instructions.Do not include any additional commentary.
                        - Be as comprehensive as possible in generating the test scenarios.
                        - Generate 20 or more test for each test category whenever possible.


                        # Instructions:

                        1. **Document Analysis**: 
                        - Begin by analyzing the provided FRD and TD documents. Identify functional or technical aspects that need to be tested.
                        - Pay closer attention to High-level and Low-level designs to identify functional or technical aspects that need to be tested.
                        - Look for any schema changes, settings and flags to generate test scenarios around it.
                        - **Review any other attached documents** (e.g., architecture diagrams, integration notes, test reports) that may provide relevant context, validations, or usage flows related to the feature or endpoint.
                        2. **Identify Test Scenarios**: Based on the document analysis, list all possible scenarios that need testing. Focus on different inputs, functional paths, and edge cases.
                        3. **Functional Test Scenarios**:
                        - **Positive Tests**: Define scenarios for expected system behavior under normal conditions.
                        - **Negative Tests**: Formulate tests for invalid inputs and unexpected user actions.
                        - **Edge Cases**: Identify and develop tests for boundary conditions and unusual scenarios.

                        4. **Non-Functional Test Scenarios**:
                        - **Performance**: Evaluate response time, scalability, and resource usage.
                        - **Security**: Test data protection and resistance to unauthorized access.
                        - **Usability**: Assess user interface ease of use and accessibility.
                        - **Other Aspects**: Look at reliability, compatibility, and maintainability.

                        5. **Scenario Description**: Write a clear and specific description for each test scenario.
                        6. **Expected Results**: Determine the expected results for each test scenario, aligning them with the objectives stated in the FRD/TD.
                        7. **JSON Structuring**: Organize the scenarios with their descriptions and expected results into a JSON format.

                        # Special Instructions:
                        {special_instructions}

                        # Output Format

                        Format the output as a JSON object with a key *strictly* named "test_list" containing a JSON list, where each list element is an object representing a test case. Each object should follow the outlined structure:
                        1. `scenarioDescription`: A string describing the test scenario.
                        2. `expectedResults`: A string detailing the expected results of that scenario.

                        Example JSON Output:
                        ```json
                        {{
                            'test_list':
                                [
                                    {{
                                        "scenarioDescription": "Scenario 1",
                                        "expectedResults": "Expected result for scenario 1"
                                    }},
                                    {{
                                        "scenarioDescription": "Scenario 2",
                                        "expectedResults": "Expected result for scenario 2"
                                    }}
                                    
                                ]
                        }}
                        ```

                        # Examples

                        **Example Input:**
                        - A snippet from an FRD or TD describing a functionality or feature.

                        **Example Output:**
                        ```json
                        {{
                            'test_list':

                                [
                                    {{
                                        "scenarioDescription": "Scenario 1",
                                        "expectedResults": "Expected result for scenario 1"
                                    }},
                                    {{
                                        "scenarioDescription": "Scenario 2",
                                        "expectedResults": "Expected result for scenario 2"
                                    }}
                                    
                                ]
                        }}
                        ```

                        # Notes

                        - In scenarios where specific documents are unavailable, use best guesses based on typical functionality.
                        - For technical tasks in engineering without an FRD, focus on the expected technical outcomes from the TD.
                        - Consider edge cases and exceptions as these often provide the most insightful test scenarios.
                        - Prioritize scenarios based on their impact on the system's functionality.

                """
    return prompt


def get_advanced_tests_prompt(user_journey, special_instructions):
    prompt = f"""Design test cases for user journey '{user_journey}' based on the provided Feature Requirement Document (FRD), Technical Design Document (TD). Focus on user journeys or key features when the TD is available. Utilize advanced test design techniques to generate a comprehensive list of test scenarios.

                    Note: 
                        - Just generate requested JSON without any other details, observations or instructions.Do not include any additional commentary.
                        - Be as comprehensive as possible in generating the test scenarios.
                        - Generate 20 or more test for each test category whenever possible.

                    # Instructions:

                    1. **Review Documents:**
                    - Thoroughly analyze the FRD to identify all intended functionalities and user stories.
                    - Examine the TD to understand the technical implementation of the key features and user journeys.
                    - Pay closer attention to High-level and Low-level designs to identify functional or technical aspects that need to be tested.
                    - Look for any schema changes, settings and flags to generate test scenarios around it.
                    - Consult the product knowledge base for additional context and historical information.
                    - **Review any other attached documents** (e.g., architecture diagrams, integration notes, test reports) that may provide relevant context, validations, or usage flows related to the feature or endpoint.

                    2. **Identify Key Features:**
                    - From the TD, pinpoint major features and user journeys critical to the application’s functionality.

                    3. **Apply Test Design Techniques:**
                    - Use advanced test design methods such as boundary value analysis, equivalence partitioning, state transition, decision table testing, and use case testing to formulate scenarios.
                    - Focus particularly on user journeys or critical features outlined in the TD.

                    4. **Define Test Scenarios:**
                    - Create test scenarios that cover all possible paths, including positive, negative, edge cases, and unexpected inputs.

                    5. **Document Scenario Descriptions:**
                    - Write clear and concise descriptions for each test scenario.

                    6. **Determine Expected Results:**
                    - Define the expected outcome for each test scenario based on the documents reviewed.

                    # Special Instructions:
                    {special_instructions}

                    # Output Format

                    Format the output as a JSON object with a key *strictly* named "test_list" containing a JSON list, where each list element is an object representing a test case. Each object should follow the outlined structure:
                        1. `scenarioDescription`: A string describing the test scenario.
                        2. `expectedResults`: A string detailing the expected results of that scenario.

                    Example JSON Output:
                        ```json
                        {{
                            'test_list':
                                [
                                    {{
                                        "scenarioDescription": "Scenario 1",
                                        "expectedResults": "Expected result for scenario 1"
                                    }},
                                    {{
                                        "scenarioDescription": "Scenario 2",
                                        "expectedResults": "Expected result for scenario 2"
                                    }}
                                    
                                ]
                        }}
                        ```

                        # Examples

                        **Example Input:**
                        - A snippet from an FRD or TD describing a functionality or feature.

                        **Example Output:**
                        ```json
                        {{
                            'test_list':

                                [
                                    {{
                                        "scenarioDescription": "Scenario 1",
                                        "expectedResults": "Expected result for scenario 1"
                                    }},
                                    {{
                                        "scenarioDescription": "Scenario 2",
                                        "expectedResults": "Expected result for scenario 2"
                                    }}
                                    
                                ]
                        }}
                        ```

                        # Notes

                        - Ensure comprehensive coverage by including various test case types: functional, performance, security, and user interface scenarios.
                        - Consider both positive and negative test cases to validate the robustness and error handling of the feature.
                        - If applicable, include scenarios for different user roles and permissions to ensure role-based access control is respected.
                        - Ensure that each scenario is aligned with the priority and criticality indicated in the documents.
                        - Include exploratory tests to discover unknown issues or behavior not specified in the FRD or TD.

                """
    return prompt


def get_microservice_component_tests_prompt(special_instructions):
    prompt = f"""
                            Generate a comprehensive list of microservices-level component test scenarios based on a given development item using a Functional Requirements Document (FRD) and Technical Design Document. For each scenario, include a detailed description and the expected results. Output the list in JSON format.

                            # Steps

                            1. **Review the FRD and Technical Design Document**: Thoroughly understand the functional requirements and the technical design of the development item.
                            - **Review any other attached documents** (e.g., architecture diagrams, integration notes, test reports) that may provide relevant context, validations, or usage flows related to the feature or endpoint.
                            2. **Identify Key Components and Interfaces**: Determine the microservices and interfaces involved in the development item.
                            3. **Derive Test Scenarios**:
                            - Analyze the requirements and design to identify all possible test scenarios.
                            - Ensure each scenario is focused on a specific function or component of the microservices.
                            4. **Describe Each Scenario**: Write a detailed description for each test scenario, outlining the setup, inputs, and functionality being tested.
                            5. **Define Expected Results**: Clearly state the expected outcomes for each scenario, based on the requirements and design.
                            6. **Compile in JSON**: Organize all scenarios in JSON format, including both Scenario Description and Expected Results.

                            # Special Instructions:
                            {special_instructions}

                            # Output Format
                            Format the output as a JSON object with a key *strictly* named "test_list" containing a JSON list, where each list element is an object representing a test case. Each object should follow the outlined structure:
                            1. `scenarioDescription`: A string describing the test scenario.
                            2. `expectedResults`: A string detailing the expected results of that scenario.                        

                            ```json
                            {{
                                'test_list':
                                    [
                                        {{
                                            "scenarioDescription": "Scenario 1",
                                            "expectedResults": "Expected result for scenario 1"
                                        }},
                                        {{
                                            "scenarioDescription": "Scenario 2",
                                            "expectedResults": "Expected result for scenario 2"
                                        }}

                                    ]
                            }}
                            ```

                            # Examples

                            Example 1:
                            - **Input**: 
                            - FRD and Technical Design Document for an development item.
                            - Development Item: Implementing a new product feature.
                            - **Output**:
                            ```json
                            {{
                                'test_list':

                                    [
                                        {{
                                            "scenarioDescription": "Scenario 1",
                                            "expectedResults": "Expected result for scenario 1"
                                        }},
                                        {{
                                            "scenarioDescription": "Scenario 2",
                                            "expectedResults": "Expected result for scenario 2"
                                        }}

                                    ]
                            }}
                            ```

                            # Notes

                            - Ensure each scenario accurately reflects the requirements and design provided.
                            - Include edge cases and potential failure modes where applicable.
                            - Scenarios should be clear, specific, and testable.

                        """
    return prompt


def get_end_to_end_tests_prompt(user_journey, special_instructions):
    prompt = f"""
                            Develop comprehensive end-to-end test scenarios using the Flow Builder for a given development item in 6sense. Use the Functional Requirements Document (FRD), 
                            Technical Design Document, and the 6sense Product Knowledgebase to ensure a thorough understanding of the feature. Ensure that all test scenarios focus on validating the feature’s
                            integration within Flow Builder and the correct execution of Flows.

                            # Development Item:
                            '{user_journey}'

                            # Special Instructions:
                                - Please generate only end-to-end scenarios involving Flow run
                                - First develop your strategy, retrieve relevant information and then execute your plan
                                - Generate 10 or more test scenarios whenever possible.


                            #Steps

                            1. **Analyze Documentation:**
                            - Review the Functional Requirements Document (FRD) to understand the feature's functional goals.
                            - Examine the Technical Design Document for detailed implementation insights.
                            - Consult the 6sense product knowledgebase for additional context and constraints.
                            - **Review any other attached documents** (e.g., architecture diagrams, integration notes, test reports) that may provide relevant context, validations, or usage flows related to the feature or endpoint.

                            2. ** Identify Flow Builder-Based Test Scenarios:**
                            - Define test scenarios that cover all functional aspects of the feature within Flow Builder.
                            - Ensure scenarios validate the execution of given development item through automation within Flows.
                            - Cover edge cases, typical use cases, and failure-handling mechanisms.

                            3.  **Describe Each Scenario:**
                            - Provide a clear and concise description of each test scenario within the Flow Builder context.
                            - Specify actions, conditions, and triggers required for Flow execution.

                            4.  **Determine Expected Results:**
                            - Clearly define what the expected outcome should be for each scenario.
                            - Include system behavior in success and failure cases.

                            # Additional Instructions:
                            {special_instructions}

                            # Output Format:

                            Format the output as a JSON object with a key *strictly* named "test_list" containing a JSON list, where each list element is an object representing a test case. Each object should follow the outlined structure:
                            1. `scenarioDescription`: A string describing the test scenario.
                            2. `expectedResults`: A string detailing the expected results of that scenario.

                            ```json
                            {{
                                'test_list':
                                        [
                                        {{
                                            "scenarioDescription": "[Detailed description of the test scenario]",
                                            "expectedResults": "[Precise expected result(s) of the scenario]"
                                        }},
                                        {{
                                            "scenarioDescription": "[Detailed description of another test scenario]",
                                            "expectedResults": "[Precise expected result(s) of the previous scenario]"
                                        }}
                                        ]
                            }}
                            ```

                            # Examples

                            ## Example Input:

                            • **Functional Requirements Document (FRD):** Defines the expected behavior, business logic, and intended use cases of the feature within Celigo’s platform. It outlines how the feature should function within Flow Builder and how users are expected to interact with it.
                            • **Technical Design Document (TDD):** Provides detailed implementation insights, including APIs, data models, configurations, and system interactions. It describes how the feature integrates with Flow Builder, how scripts or automation logic are executed, and any dependencies or architectural considerations.
                            • **6sense Product Knowledgebase:**  Contains best practices, existing platform capabilities, and system constraints related to Flow automation, scripting, permissions, error handling, and system behavior. It serves as a reference for how the feature should operate within Celigo’s iPaaS environment.

                            ```json
                            {{
                                'test_list':
                                    [
                                    {{
                                        "scenarioDescription": "[Detailed description of the test scenario]",
                                        "expectedResults": "[Precise expected result(s) of the scenario]"
                                    }},
                                    {{
                                        "scenarioDescription": "[Detailed description of another test scenario]",
                                        "expectedResults": "[Precise expected result(s) of the previous scenario]"
                                    }}
                                    ]
                            }}
                            ```

                            # Notes:
                            - Generate end-to-end scenarios using **Flow**
                            - Ensure each test scenario is explicitly tied to Flow Builder.
                            - Ensure each scenario's expected results are realistic and achievable with the given feature design.
                            - Include additional scenarios to cover all potential use cases and exceptions identified through documentation review.
                            - Verify that all critical backend integrations within the feature's wider system operation are covered.
                    """
    return prompt


def get_sentence_case_prompt(user_journey, special_instructions, acceptance_criteria):
    prompt = f"""
                            Generate a comprehensive list of Sentence case test scenarios for for ** User journey '{user_journey}'** mentioned in functional requirements document (FRD), technical design document (TDD) and Figma images. 
                            Leverage the 6sense product knowledge base to gain a detailed understanding of any product feature.

                            Ensure each scenario includes a description and expected results. Compile the output in a JSON format consisting of these details.


                            Note: 
                            - Just generate requested JSON without any other details, observations or instructions. Do not include any additional commentary.
                            - Be as comprehensive as possible in generating the test scenarios.
                            - Generate 10 or more test for each test category whenever possible.


                            # Steps

                            1. **Document Analysis**: Begin by analyzing the attached FRD,  TDD or Figma images. Identify functional or technical aspects that need to be tested.
                            - **Review any other attached documents** (e.g., architecture diagrams, integration notes, test reports) that may provide relevant context, validations, or usage flows related to the feature or endpoint.
                            2. **Identify Test Scenarios**: Based on the document analysis, list all possible scenarios that need testing. Focus on different inputs, functional paths, and edge cases.
                            3. **Test Objective Alignment**: Ensure that the test scenarios align with the objectives outlined in the FRD or TDD.
                            4. **Focus on Test Scenarios for Sentence case Test**: Identify comprehensive test scenarios for the Sentence case Test category. Consider the following aspects:
                            - **Sentence Case Testing**: When ever new UI elements are added to Integrator.IO UI, we want to make sure that all the visible text like Labels etc. are following sentence case. You can scan the attached documents to figure out newly introduced UI elements.
                            - Take help from attached Figma images in PDF.
                            - Focus on Positive, Negative  and Edge Cases scenarios.

                            5. **Scenario Description**: Write a clear and specific description for each test scenario.
                            6. **Expected Results**: Determine the expected results for each test scenario, aligning them with the objectives stated in the FRD/TDD.
                            7. **JSON Structuring**: Organize the scenarios with their descriptions and expected results into a JSON format.

                            # Special Instructions:
                            {special_instructions}

                            # Output Format

                            Format the output as a JSON object with a key *strictly* named "test_list" containing a JSON list, where each list element is an object representing a test case. Each object should follow the outlined structure:
                            1. `scenarioDescription`: A string describing the test scenario.
                            2. `expectedResults`: A string detailing the expected results of that scenario.

                            {acceptance_criteria}

                            Example JSON Output:
                            ```json
                            {{
                                'test_list':
                                    [
                                        {{
                                            "scenarioDescription": "Scenario 1",
                                            "expectedResults": "Expected result for scenario 1"
                                        }},
                                        {{
                                            "scenarioDescription": "Scenario 2",
                                            "expectedResults": "Expected result for scenario 2"
                                        }}

                                    ]
                            }}
                            ```

                            # Examples

                            **Example Input:**
                            - A snippet from an FRD or TDD describing a functionality or feature.

                            **Example Output:**
                            ```json
                            {{
                                'test_list':

                                    [
                                        {{
                                            "scenarioDescription": "Scenario 1",
                                            "expectedResults": "Expected result for scenario 1"
                                        }},
                                        {{
                                            "scenarioDescription": "Scenario 2",
                                            "expectedResults": "Expected result for scenario 2"
                                        }}

                                    ]
                            }}
                            ```

                            # Notes

                            - In scenarios where specific documents are unavailable, use best guesses based on typical functionality.
                            - For technical tasks in engineering without an FRD, focus on the expected technical outcomes from the TDD (Technical Design Document).
                            - Consider edge cases and exceptions as these often provide the most insightful test scenarios.
                            - Prioritize scenarios based on their impact on the system's functionality.
                    """
    return prompt


def get_ui_ux_cross_browser_backward_compatability_prompt(test_type, user_journey, special_instructions,
                                                          acceptance_criteria):
    prompt = f"""Generate a comprehensive list of {test_type} scenarios for ** User journey '{user_journey}'** mentioned in Feature requirements document (FRD) and technical design document (TD).

                                Ensure each scenario includes a description and expected results. Compile the output in a JSON format consisting of these details.


                                Note: 
                                - Just generate requested JSON without any other details, observations or instructions. Do not include any additional commentary.
                                - Be as comprehensive as possible in generating the test scenarios.
                                - Generate 20 or more test for each test category whenever possible.


                                # Instructions:

                                1. **Document Analysis**: 
                                - Begin by analyzing the provided FRD or TD. 
                                - Pay closer attention to High-level and Low-level designs to identify functional or technical aspects that need to be tested.
                                - Look for any schema changes, settings and flags to generate test scenarios around it.
                                - **Review any other attached documents** (e.g., architecture diagrams, integration notes, test reports) that may provide relevant context, validations, or usage flows related to the feature or endpoint.
                                2. **Identify Test Scenarios**: Based on the document analysis, list all possible scenarios that need testing. Focus on different inputs, functional paths, and edge cases.
                                3. **Test Objective Alignment**: Ensure that the test scenarios align with the objectives outlined in the FRD or TD.
                                4. **Focus on Test Scenarios for {test_type}**: Identify comprehensive test scenarios for the {test_type} category. Consider the following aspects:
                                - Focus on Positive, Negative and Edge Cases scenarios.
                                5. **Scenario Description**: Write a clear and specific description for each test scenario.
                                6. **Expected Results**: Determine the expected results for each test scenario, aligning them with the objectives stated in the FRD/TD.
                                7. **JSON Structuring**: Organize the scenarios with their descriptions and expected results into a JSON format.

                                # Special Instructions:
                                {special_instructions}

                                {acceptance_criteria}

                                # Output Format

                                Format the output as a JSON object with a key *strictly* named "test_list" containing a JSON list, where each list element is an object representing a test case. Each object should follow the outlined structure:
                                1. `scenarioDescription`: A string describing the test scenario.
                                2. `expectedResults`: A string detailing the expected results of that scenario.

                                Example JSON Output:
                                ```json
                                {{
                                    'test_list':
                                        [
                                            {{
                                                "scenarioDescription": "Scenario 1",
                                                "expectedResults": "Expected result for scenario 1"
                                            }},
                                            {{
                                                "scenarioDescription": "Scenario 2",
                                                "expectedResults": "Expected result for scenario 2"
                                            }}

                                        ]
                                }}
                                ```
                                 **Example Output:**
                            ```json
                            {{
                                'test_list':

                                    [
                                        {{
                                            "scenarioDescription": "User logs in with valid credentials",
                                            "expectedResults": "User is directed to the dashboard"
                                        }},
                                        {{
                                            "scenarioDescription": "User attempts to log in with invalid credentials",
                                            "expectedResults": "Error message indicating incorrect username or password appears"
                                        }}

                                    ]
                            }}
                            ```

                            # Notes

                            - In scenarios where specific documents are unavailable, use best guesses based on typical functionality.
                            - For technical tasks in engineering without an FRD, focus on the expected technical outcomes from the TDD (Technical Design Document).
                            - Consider edge cases and exceptions as these often provide the most insightful test scenarios.
                            - Prioritize scenarios based on their impact on the system's functionality.

                """
    return prompt


def get_default_else_prompt(test_type, user_journey, special_instructions):
    prompt = f"""Generate a comprehensive list of {test_type} scenarios for user journey '{user_journey}' using the Feature requirements document (FRD) and technical design document (TD).

                                Ensure each scenario includes a description and expected results. Compile the output in a JSON format consisting of these details.


                                Note: 
                                - Just generate requested JSON without any other details, observations or instructions. Do not include any additional commentary.
                                - Be as comprehensive as possible in generating the test scenarios.
                                - Generate 20 or more test for each test category whenever possible.


                                # Instructions:

                                1. **Document Analysis**: 
                                - Begin by analyzing the provided FRD or TD. 
                                - Pay closer attention to High-level and Low-level designs to identify functional or technical aspects that need to be tested.
                                - Look for any schema changes, settings and flags to generate test scenarios around it.
                                - **Review any other attached documents** (e.g., architecture diagrams, integration notes, test reports) that may provide relevant context, validations, or usage flows related to the feature or endpoint.
                                2. **Identify Test Scenarios**: Based on the document analysis, list all possible scenarios that need testing. Focus on different inputs, functional paths, and edge cases.
                                3. **Test Objective Alignment**: Ensure that the test scenarios align with the objectives outlined in the FRD or TD.
                                4. **Focus on Test Scenarios for {test_type}**: Identify comprehensive test scenarios for the {test_type} category. Consider the following aspects:
                                - Focus on Positive, Negative and Edge Cases scenarios.
                                5. **Scenario Description**: Write a clear and specific description for each test scenario.
                                6. **Expected Results**: Determine the expected results for each test scenario, aligning them with the objectives stated in the FRD/TD.
                                7. **JSON Structuring**: Organize the scenarios with their descriptions and expected results into a JSON format.

                                # Special Instructions:
                                {special_instructions}

                                # Output Format

                                Format the output as a JSON object with a key *strictly* named "test_list" containing a JSON list, where each list element is an object representing a test case. Each object should follow the outlined structure:
                                1. `scenarioDescription`: A string describing the test scenario.
                                2. `expectedResults`: A string detailing the expected results of that scenario.

                                Example JSON Output:
                                ```json
                                {{
                                    'test_list':
                                        [
                                            {{
                                                "scenarioDescription": "Scenario 1",
                                                "expectedResults": "Expected result for scenario 1"
                                            }},
                                            {{
                                                "scenarioDescription": "Scenario 2",
                                                "expectedResults": "Expected result for scenario 2"
                                            }}

                                        ]
                                }}
                                ```

                                # Examples

                                **Example Input:**
                                - A snippet from an FRD or TD describing a functionality or feature.

                                **Example Output:**
                                ```json
                                {{
                                    'test_list':

                                        [
                                            {{
                                                "scenarioDescription": "Scenario 1",
                                                "expectedResults": "Expected result for scenario 1"
                                            }},
                                            {{
                                                "scenarioDescription": "Scenario 2",
                                                "expectedResults": "Expected result for scenario 2"
                                            }}

                                        ]
                                }}
                                ```

                                # Notes

                        - Ensure comprehensive coverage by including various test case types: functional, performance, security, and user interface scenarios.
                        - Consider both positive and negative test cases to validate the robustness and error handling of the feature.
                        - If applicable, include scenarios for different user roles and permissions to ensure role-based access control is respected.
                        - Ensure that each scenario is aligned with the priority and criticality indicated in the documents.
                        - Include exploratory tests to discover unknown issues or behavior not specified in the FRD or TD.
                        """
    return prompt


def get_quality_check_feedback_prompt(answer):
    prompt = (
        "You are a quality assurance engineer. Checking quality of automated extraction generated by LLMs and giving human feedback."
        "Here in this step we are trying to ger full list of resources for Salesforce B2B and D2C Commerce API, starting from 'Commerce Extension Mapping' and ending at 'Commerce Webstore Shipment Items'."
        "Quality check LLM answer given below and generate feedback for the LLM. If you think LLM answer is complete say 'Finished', else generate followup question for the LLM chat session."
        f'\n LLM Answer: "{answer}"'
    )
    return prompt


def get_ia_automation_prompt(test_case_title="", test_script=""):
    prompt = f"""You are helping automate test cases for Celigo’s **Shopify ↔ NetSuite Integration App (IA)** using the **IA Automation Framework**.
Your task: **Enhance and review each step by replacing any backend or data-setup action** (e.g., order creation, record edits in Shopify or NetSuite) with the **specific, documented API method** from *IA Automation Framework API Documentation.pdf*.
---
### **Key Guidelines**
1. **Use test automation methods for test data setup in Shopify/NetSuite:**
   - If a step creates, updates, or manipulates records in Shopify or NetSuite (e.g., orders, customers, products, records), use the automation method from the IA Automation Framework.
   - You must search the *IA Automation Framework API Documentation.pdf* for actual function names and use exact syntax/parameter conventions.
   - Do NOT invent function names. If no suitable method exists, mark the step as [MANUAL DATA SETUP].
2. **[UI SETTING] for 6sense IA configuration steps:**
   - Only prefix with [UI SETTING] if the step references **6sense Integration App’s UI**—for example, actions involving Tabs, menus, checkboxes, pickers, search/lookups, or dropdowns in the **6sense IA** admin/config interface.
3. **Never automate UI actions or browser flows:**
   - Do not replace UI/config steps with automation. Only use [UI SETTING] for these, and clarify or improve manual wording if necessary.
4. **Be specific about method usage:**
   - When replacing a step with automation, specify the automation function and key input params.
   - If multiple methods might suit, select the most appropriate or provide a brief note to guide selection (e.g., “use `createSHPFOrderthoughAPI` for confirmed order creation”).
5. **Do not automate UI test flows or assertions.**
   - Only actions that interact with backend records or configurations via API are to be automated.
6. **Never automate 6sense IA UI actions, only **Shopify/NetSuite/Backend data actions**.
---

Below is a test case for review and enhancement.
---
## Test Case Description  
**{test_case_title}**

## Manual Test Steps  
{test_script}

---

### **Output Format**
Return the reviewed and enhanced steps **numbered in order**, with **[UI SETTING]** only on 6sense IA UI steps and backend actions replaced with exact automation methods.  
If you cannot find an automation method, cite as **[MANUAL DATA SETUP]**.
Enclose the output in the following JSON structure:
```json
{{
  "reviewed_steps_list": [
    "<Step 1>",
    "<Step 2>",
    ...
  ]
}}

Notes
	• Always cross-reference IA Automation Framework API Documentation.pdf for accuracy.
	• Clarify any step as needed for logic, sequence, and completeness.

"""
    return prompt


def get_ia_automation_reflection_prompt():
    prompt = """You are a quality assurance engineer reviewing IA automation test case enhancements.

Your task is to evaluate whether the test case automation review is complete and accurate.

Check for:
1. **Completeness**: Are all manual steps reviewed and properly categorized?
2. **Automation accuracy**: Are backend/API actions correctly mapped to IA Automation Framework methods?
3. **UI identification**: Are UI settings properly identified and prefixed with [UI SETTING]?
4. **JSON format**: Is the output in the correct JSON format with 'reviewed_steps_list'?
5. **Step numbering**: Are steps numbered in the same order as the original?

If the review is complete and accurate, set Finished to true. Otherwise, provide specific feedback to improve the automation review.
"""
    return prompt
