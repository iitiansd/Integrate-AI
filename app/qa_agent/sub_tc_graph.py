### Code generated by Github Copilot ###

from langchain.schema import SystemMessage, HumanMessage, BaseMessage, AIMessage
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnableMap
from langchain_openai import ChatOpenAI
from langchain.callbacks import get_openai_callback

from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

from qa_agent.cl_agent import OpenAIAssistantExecuters

import requests

from langchain_core.pydantic_v1 import BaseModel, Field, ValidationError
from langchain.output_parsers.openai_tools import (
    JsonOutputToolsParser,
    PydanticToolsParser,
)

import asyncio

from typing import List, Sequence, TypedDict, Annotated, Optional, Literal, Union
import operator

from langgraph.graph import END, MessageGraph, StateGraph
from langgraph.checkpoint.memory import MemorySaver

from openai import OpenAI

from pprint import pprint
import os
import time
import json  # <-- Move this import to the top-level with other imports

from .prompts.tc_graph_prompts import (
    get_qa_reflection_stage1_prompt,
    get_qa_reflection_stage2_prompt,
    get_qa_reflection_stage3_prompt,
    get_general_test_case_generation_prompt,
    get_backend_test_case_generation_prompt,
    get_front_end_test_case_generation_prompt,
    get_test_case_type_prompt_soft,
    get_test_case_type_prompt_hard,
    get_qa_sub_reflection_stage1_prompt
)

import traceback  # Import traceback for detailed error logging

MAX_REVISION = 3  # Define maximum revisions globally

# Add a global variable for thread_id at the top (similar to tc_graph.py)
stage1_thread_id = None

class SubAutoconState(TypedDict):
    test_list: list[dict]  # list of test cases with test_type included
    current_test_index: int  # index of the current test case
    message_history: Annotated[list[BaseMessage], operator.add]
    is_finished: bool
    revisions: int
    is_test_list_processed: bool  # Flag to track if test_list processing is finished
    attachments: Optional[list[dict]]  # Add attachments as an optional list of dictionaries

# Define the reflection model and prompt
qa_reflection_model = "gpt-4-turbo"
llm = ChatOpenAI(temperature=0.0, model_name=qa_reflection_model)

class Reflection(BaseModel):
    """Reflection and Followup"""
    Finished: bool = Field(description="Based upon your reflection decide whether task is finished or not.")
    follow_up_question: str = Field(
        description="Follow up question to LLM chat which should encourage LLM to finish the task, when you think it's not finished."
    )
    reasonings: str = Field(description="Reasoning for your reflection and follow-up question.")

qa_reflection_prompt_stage1 = ChatPromptTemplate.from_messages(
    [
        ("system", get_qa_sub_reflection_stage1_prompt()),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

qa_reflect_stage1 = qa_reflection_prompt_stage1 | llm.bind_tools(tools=[Reflection], tool_choice="Reflection")

class SubQAGraph:
    def __init__(self, max_revisions: int = 3):
        self.max_revisions = max_revisions
        self.cumulative_usage_reflection = {
            "completion_tokens": 0,
            "prompt_tokens": 0,
            "total_tokens": 0,
            "total_cost": 0.0,
        }

    def _calculate_cumulative_usage_reflection(self, cb, usage):
        if cb:
            usage["completion_tokens"] += cb.completion_tokens
            usage["prompt_tokens"] += cb.prompt_tokens
            usage["total_tokens"] += cb.total_tokens
            usage["total_cost"] += cb.total_cost
            print(
                f"Cumulative Usage Reflection: Completion tokens: {usage['completion_tokens']}, "
                f"Prompt tokens: {usage['prompt_tokens']}, Total tokens: {usage['total_tokens']}, "
                f"Total cost: {usage['total_cost']}"
            )
        return usage

    def _sub_assist_stage1_node(self, state: SubAutoconState):
        global stage1_thread_id  # Use the global variable

        # Ensure 'attachments' key exists in the state
        if "attachments" not in state:
            state["attachments"] = []  # Initialize as an empty list if not present

        # Adjust index to process the correct test case
        current_test_idx = state['current_test_index'] - 1  # Convert 1-based index to 0-based
        print(f"Processing test case at index {current_test_idx}: {state['test_list'][current_test_idx]}")

        # Check if this is a revision or a new test case processing
        is_revision = state.get('revisions', 0) > 0

        if is_revision:
            # For revision, use the follow_up_question as the query
            query = state['message_history'][-1].content
        else:
            # For new test case processing, generate a query for the current test
            current_test = state['test_list'][current_test_idx]  # Get the current test
            print(f"Current test: {current_test}")
            current_test_details = None
            if isinstance(current_test, tuple) and len(current_test) == 2 and isinstance(current_test[1], dict) and 'Title' in current_test[1]:
                current_test_details = current_test[1]
            print(f"Current test details: {current_test_details}")

            if current_test_details:
                query = get_test_case_type_prompt_hard(current_test_details)
            else:
                query = "Invalid test details provided."

        if not query:
            query = "No query generated due to missing or invalid test details."

        query_message = HumanMessage(content=query)

        assist = OpenAIAssistantExecuters(agent_id="asst_lYJEAxz8st4RMEWC7Na5fuwM")  # Replace with actual assistant ID
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        try:
            # Include attachments in the query if available
            query_input = {"promptInput": {"query": query}}
            if state["attachments"]:  # Check if attachments are not empty
                query_input["attachments"] = state["attachments"]

            # Only pass threadID if this is a revision
            if is_revision and stage1_thread_id is not None:
                query_input["threadID"] = stage1_thread_id

            out = loop.run_until_complete(assist.get_query_chain(query_input))
            test_list_copy = state['test_list'][:]  # Initialize test_list_copy with a shallow copy
            print(f"Assistant output: {out}")
            if out['agent_output'] and out['agent_output']['thread_id']:
                # Capture thread_id in the global variable
                stage1_thread_id = out['agent_output']['thread_id']
                # Handle output with 'query' key
                if out['query'] and isinstance(out['query'], dict):
                    output = out['query']
                    test_type = output.get('Test_Type')
                    reason = output.get('Reason', "No reason provided.")
                    # Place the actual JSON string as the AI message content
                    out_message = AIMessage(content=json.dumps(output))
                    
                    # Unpack the tuple, modify the dictionary, and reassemble the tuple
                    index, test_details = test_list_copy[current_test_idx]
                    test_details['test_type'] = test_type
                    test_details['Reason'] = reason
                    test_list_copy[current_test_idx] = (index, test_details)
                else:
                    print(f"Unexpected query format: {type(out['query'])}. Expected a dictionary.")
                    test_type = "Error"
                    reason = "Invalid query format."
                    out_message = AIMessage(content="**ERROR parsing JSON:** Failed to process output as proper JSON 'Test_Type' and 'Reason' key not found, Received following output response, probably with incorrect JSON.\n" +
                                out['agent_output']['output'])
            else:
                test_type = "Error"
                reason = "Invalid JSON or thread"
                out_message = AIMessage(content="Failed to generate test type and reason.")
        except Exception as e:
            print(f"Error in _sub_assist_stage1_node: {e}")
            traceback.print_exc()  # Print the complete stack trace
            test_type = "Error"
            reason = "OpenAI server error"
            test_list_copy = state['test_list'][:]  # Ensure test_list_copy is initialized in case of an exception
            out_message = AIMessage(content="Server error occurred while generating the answer. Please try again.")

        # Update the test_list with the new test type and reason
        index, test_details = test_list_copy[current_test_idx]
        test_details['test_type'] = test_type
        test_details['Reason'] = reason
        test_list_copy[current_test_idx] = (index, test_details)

        return {
            "message_history": [query_message, out_message],  # Directly return the messages
            "test_list": test_list_copy,
        }

    def _sub_qa_reflection_stage1_node(self, state: SubAutoconState):
        # skip message and reset revisions if max revisions reached
        revisions = state.get('revisions', 0)
        if revisions >= self.max_revisions:
            next_test_index = state.get('current_test_index', 1) + 1
            is_test_list_processed = next_test_index > len(state['test_list'])
            return {
                "revisions": 0,
                "is_finished": True,
                "current_test_index": next_test_index,
                "is_test_list_processed": is_test_list_processed,
            }

        # Extract the last two messages from the history
        messages = state['message_history'][-2 * (revisions + 1):]

        # Invoke the reflection stage with the extracted messages
        with get_openai_callback() as cb:
            try:
                res = qa_reflect_stage1.invoke({"messages": messages})
            except Exception as e:
                print(f"Error in _sub_qa_reflection_stage1_node: {e}")
                return {
                    "message_history": [AIMessage(content="Reflection failed.")],  # Directly return the message
                    "revisions": state.get('revisions', 0),
                    "is_finished": False
                }

        # Log usage statistics for reflection
        self.cumulative_usage_reflection = self._calculate_cumulative_usage_reflection(cb, self.cumulative_usage_reflection)

        # Parse the reflection output
        parser = JsonOutputToolsParser(return_id=True)
        tool_invocation: AIMessage = res
        parsed_tool_calls = parser.invoke(tool_invocation)

        # Extract reflection results
        is_finished = parsed_tool_calls[0]['args'].get('Finished', False)
        followup_question = parsed_tool_calls[0]['args'].get(
            'follow_up_question', 
            "PLEASE RETRY THE TASK. "
        )

        # Update revision count and prepare the next question
       
        if is_finished:
            revisions = 0
            question = AIMessage(content="Finished processing the test case.")
            next_test_index = state.get('current_test_index', 1) + 1
            is_test_list_processed = next_test_index > len(state['test_list'])
            return {
                "message_history": [question],
                "revisions": revisions,
                "is_finished": is_finished,
                "current_test_index": next_test_index,
                "is_test_list_processed": is_test_list_processed,
            }
        else:
            revisions += 1
            question = HumanMessage(content=followup_question)
            return {
                "message_history": [question],
                "revisions": revisions,
                "is_finished": is_finished               
            }

    def _sub_should_continue(self, state: SubAutoconState):
        if state.get('is_test_list_processed', False):
            print("All test cases processed. Ending graph.")
            return END  # End the graph if all test cases are processed
        return "sub_assist_stage1"  # Continue processing the next test case

    def prepare_graph(self) -> StateGraph:
        builder = StateGraph(SubAutoconState)
        builder.add_node("sub_assist_stage1", self._sub_assist_stage1_node)
        builder.add_node("sub_reflect_stage1", self._sub_qa_reflection_stage1_node)

        builder.set_entry_point("sub_assist_stage1")
        builder.add_edge("sub_assist_stage1", "sub_reflect_stage1")
        builder.add_conditional_edges("sub_reflect_stage1", self._sub_should_continue)

        return builder